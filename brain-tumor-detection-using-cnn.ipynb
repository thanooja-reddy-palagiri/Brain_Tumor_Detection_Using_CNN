{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7251,"sourceType":"datasetVersion","datasetId":2798},{"sourceId":377107,"sourceType":"datasetVersion","datasetId":165566}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Brain Tumor Detection Using CNN\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-11-16T06:59:50.636877Z","iopub.execute_input":"2025-11-16T06:59:50.637415Z","iopub.status.idle":"2025-11-16T06:59:50.641246Z","shell.execute_reply.started":"2025-11-16T06:59:50.637381Z","shell.execute_reply":"2025-11-16T06:59:50.640390Z"}}},{"cell_type":"code","source":"!pip install efficientnet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:40:57.825705Z","iopub.execute_input":"2025-11-24T06:40:57.826311Z","iopub.status.idle":"2025-11-24T06:41:01.740987Z","shell.execute_reply.started":"2025-11-24T06:40:57.826288Z","shell.execute_reply":"2025-11-24T06:41:01.740253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf \nfrom sklearn.utils import shuffle\nimport glob\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Input,Conv2D, Dense, Flatten ,Dropout ,MaxPooling2D,BatchNormalization,GlobalAveragePooling2D\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam,RMSprop \nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input \nfrom tensorflow.keras import regularizers\nfrom PIL import Image\nfrom efficientnet.tfkeras import EfficientNetB0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:42:09.776056Z","iopub.execute_input":"2025-11-24T06:42:09.776774Z","iopub.status.idle":"2025-11-24T06:42:09.782068Z","shell.execute_reply.started":"2025-11-24T06:42:09.776746Z","shell.execute_reply":"2025-11-24T06:42:09.781303Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(42)\nrandom.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:42:10.385663Z","iopub.execute_input":"2025-11-24T06:42:10.386300Z","iopub.status.idle":"2025-11-24T06:42:10.390021Z","shell.execute_reply.started":"2025-11-24T06:42:10.386273Z","shell.execute_reply":"2025-11-24T06:42:10.389308Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocessing and Visualization","metadata":{}},{"cell_type":"code","source":"def load_images(folders, label_map):\n    # creating two lists to store the images and labels\n    images = []\n    labels = []\n    \n    # loading the images from each folder in the dataset\n    for folder in folders:\n        for category in os.listdir(folder):\n            category_path = os.path.join(folder, category)\n            if os.path.isdir(category_path):\n                if category in label_map:  # Check if the category is present in the label_map\n                    label = label_map[category]\n                    file_list = os.listdir(category_path)\n                    for filename in file_list:\n                        img_path = os.path.join(category_path, filename)\n                        image = cv2.imread(img_path)\n                        # resizing the images to create a standard and so that it can be suitable for the model input\n                        image = cv2.resize(image, (224, 224))\n                        # cv2 reads the image as BGR so we need to convert it back to RGB\n                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                        images.append(image)\n                        labels.append(label)\n    \n    return np.array(images), np.array(labels)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:42:12.971264Z","iopub.execute_input":"2025-11-24T06:42:12.971823Z","iopub.status.idle":"2025-11-24T06:42:12.976971Z","shell.execute_reply.started":"2025-11-24T06:42:12.971798Z","shell.execute_reply":"2025-11-24T06:42:12.976327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_folders = [\n    '/kaggle/input/brain-mri-images-for-brain-tumor-detection',\n]\n\n# encoding the labels \nlabel_map = {'no': 0, 'yes': 1}  # Map negative to 0 (no) and positive to 1 (yes)\n\nimages, labels = load_images(image_folders, label_map)\n\nprint(\"Shape of images:\", images.shape)\nprint(\"Shape of labels:\", labels.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:42:57.310983Z","iopub.execute_input":"2025-11-24T06:42:57.311650Z","iopub.status.idle":"2025-11-24T06:42:59.292155Z","shell.execute_reply.started":"2025-11-24T06:42:57.311627Z","shell.execute_reply":"2025-11-24T06:42:59.291423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\n\n# Display tumor images with label 'yes'\nfor i in range(3):\n    plt.subplot(2, 3, i+1)\n    plt.imshow(images[labels == 1][i])  # Filter images with label 'yes'\n    plt.title(\"Tumor: Yes\")\n    plt.axis('off')\n\n# Display no_tumor images with label 'no'\nfor i in range(3):\n    plt.subplot(2, 3, i+4)\n    plt.imshow(images[labels == 0][i])  # Filter images with label 'no'\n    plt.title(\"Tumor: No\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:43:02.971504Z","iopub.execute_input":"2025-11-24T06:43:02.971977Z","iopub.status.idle":"2025-11-24T06:43:03.653435Z","shell.execute_reply.started":"2025-11-24T06:43:02.971947Z","shell.execute_reply":"2025-11-24T06:43:03.652703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Counting the occurrences of each class label\nunique_labels, label_counts = np.unique(labels, return_counts=True)\n\nplt.figure(figsize=(10,5))\nplt.bar(unique_labels, label_counts, color=['blue', 'orange'])\nplt.xticks(unique_labels, ['No Tumor', 'Tumor'])\nplt.xlabel('Class Label')\nplt.ylabel('Count')\nplt.title('Distribution of Class Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:43:25.342654Z","iopub.execute_input":"2025-11-24T06:43:25.343194Z","iopub.status.idle":"2025-11-24T06:43:25.471205Z","shell.execute_reply.started":"2025-11-24T06:43:25.343169Z","shell.execute_reply":"2025-11-24T06:43:25.470533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def crop_brain_region(image, size):\n    \n    # Converting the image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # Applying Gaussian blur to smooth the image and reduce noise\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # Thresholding the image to create a binary mask\n    thresh = cv2.threshold(gray, 45, 255, cv2.THRESH_BINARY)[1]\n    \n    # Performing morphological operations to remove noise\n    thresh = cv2.erode(thresh, None, iterations=2)\n    thresh = cv2.dilate(thresh, None, iterations=2)\n    \n    # Finding contours in the binary mask\n    contours, _ = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    # Assuming the brain part of the image has the largest contour\n    c = max(contours, key=cv2.contourArea)\n    \n    # Getting the bounding rectangle of the brain part\n    x, y, w, h = cv2.boundingRect(c)\n    \n    # Drawing contours on the original image\n    contour_image = cv2.drawContours(image.copy(), [c], -1, (0, 255, 0), 2)\n    \n    # Drawing bounding box on the original image\n    bounding_box_image = cv2.rectangle(image.copy(), (x, y), (x + w, y + h), (0, 255, 0), 2)\n    \n    # Cropping the image around the bounding rectangle\n    cropped_image = image[y:y+h, x:x+w]\n    \n    # Resizing cropped image to the needed size\n    resized_image = cv2.resize(cropped_image, size)\n    \n    return contour_image, bounding_box_image, cropped_image, resized_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:43:28.661260Z","iopub.execute_input":"2025-11-24T06:43:28.661972Z","iopub.status.idle":"2025-11-24T06:43:28.668130Z","shell.execute_reply.started":"2025-11-24T06:43:28.661948Z","shell.execute_reply":"2025-11-24T06:43:28.667302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_size = (224, 224)\n\nexample_image = cv2.imread('/kaggle/input/brain-mri-images-for-brain-tumor-detection/yes/Y1.jpg')\nexample_image = cv2.cvtColor(example_image, cv2.COLOR_BGR2RGB)\n\ncontour_image, bounding_box_image, cropped_image, resized_image = crop_brain_region(example_image, output_size)\n\n\nplt.figure(figsize=(15, 10))\n\nplt.subplot(2, 2, 1)\nplt.imshow(contour_image)\nplt.title(\"Contour\")\n\nplt.subplot(2, 2, 2)\nplt.imshow(bounding_box_image)\nplt.title(\"Bounding Box\")\n\nplt.subplot(2, 2, 3)\nplt.imshow(cropped_image)\nplt.title(\"Cropped\")\n\nplt.subplot(2, 2, 4)\nplt.imshow(resized_image)\nplt.title(\"Resized\")\n\nplt.tight_layout()\nplt.show()\n\nall_cropped = []\n\n# Applying the crop function to each one of our images\nfor image in images:\n    _, _, _, resized_image = crop_brain_region(image, output_size)\n    all_cropped.append(resized_image)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:43:39.740939Z","iopub.execute_input":"2025-11-24T06:43:39.741270Z","iopub.status.idle":"2025-11-24T06:43:40.885927Z","shell.execute_reply.started":"2025-11-24T06:43:39.741248Z","shell.execute_reply":"2025-11-24T06:43:40.885090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Comparing images before and after cropping\n\nnum_images_per_class = 5\n\nclass_0_counter = 0\nclass_1_counter = 0\n\nplt.figure(figsize=(20, 10))\n\nfor i in range(num_images_per_class):\n    plt.subplot(2, num_images_per_class, i + 1)\n    plt.imshow(images[i])\n    plt.title(\"Original Image\")\n    plt.axis(\"off\")\n\nfor i in range(num_images_per_class):\n    plt.subplot(2, num_images_per_class, num_images_per_class + i + 1)\n    plt.imshow(all_cropped[i])\n    plt.title(\"Cropped Image\")\n    plt.axis(\"off\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:43:46.295952Z","iopub.execute_input":"2025-11-24T06:43:46.296266Z","iopub.status.idle":"2025-11-24T06:43:47.081895Z","shell.execute_reply.started":"2025-11-24T06:43:46.296244Z","shell.execute_reply":"2025-11-24T06:43:47.081074Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Splitting","metadata":{}},{"cell_type":"code","source":"#Now, we split the data into training, validation, and test sets. But first we have to transform our all_cropped list into a numpy array.\n\nall_cropped=np.array(all_cropped)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:00.235562Z","iopub.execute_input":"2025-11-24T06:44:00.236090Z","iopub.status.idle":"2025-11-24T06:44:00.251808Z","shell.execute_reply.started":"2025-11-24T06:44:00.236068Z","shell.execute_reply":"2025-11-24T06:44:00.250962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(all_cropped, labels, test_size=0.2,shuffle=True, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:02.061164Z","iopub.execute_input":"2025-11-24T06:44:02.061460Z","iopub.status.idle":"2025-11-24T06:44:02.078218Z","shell.execute_reply.started":"2025-11-24T06:44:02.061439Z","shell.execute_reply":"2025-11-24T06:44:02.077622Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"X_train shape:\", X_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_val shape:\", y_val.shape)\nprint(\"y_test shape:\", y_test.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:05.395430Z","iopub.execute_input":"2025-11-24T06:44:05.395949Z","iopub.status.idle":"2025-11-24T06:44:05.400840Z","shell.execute_reply.started":"2025-11-24T06:44:05.395929Z","shell.execute_reply":"2025-11-24T06:44:05.400187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_folder = 'Train'\nval_folder = 'Validation'\ntest_folder = 'Test'\n\nos.makedirs(train_folder, exist_ok=True)\nos.makedirs(val_folder, exist_ok=True)\nos.makedirs(test_folder, exist_ok=True)\n\nlabel_map_decoded = {1: 'yes', 0: 'no'}\n\ndef copy_images_to_folder(images, labels, folder):\n    for i, (image, label) in enumerate(zip(images, labels)):\n        class_name = label_map_decoded[label]\n        class_folder = os.path.join(folder, class_name)\n        os.makedirs(class_folder, exist_ok=True)\n        img_filename = f'{class_name}_{i}.jpg'  # Assuming images are in JPG format\n        img_path = os.path.join(class_folder, img_filename)\n        cv2.imwrite(img_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))  # Save image directly without converting to PIL format\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:08.145792Z","iopub.execute_input":"2025-11-24T06:44:08.146143Z","iopub.status.idle":"2025-11-24T06:44:08.152593Z","shell.execute_reply.started":"2025-11-24T06:44:08.146120Z","shell.execute_reply":"2025-11-24T06:44:08.151790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"copy_images_to_folder(X_train, y_train, train_folder)\ncopy_images_to_folder(X_val, y_val, val_folder)\ncopy_images_to_folder(X_test, y_test, test_folder)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:10.650366Z","iopub.execute_input":"2025-11-24T06:44:10.650942Z","iopub.status.idle":"2025-11-24T06:44:10.746612Z","shell.execute_reply.started":"2025-11-24T06:44:10.650916Z","shell.execute_reply":"2025-11-24T06:44:10.746028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(np.max(X_train))\nprint(np.min(X_train))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:12.820539Z","iopub.execute_input":"2025-11-24T06:44:12.821166Z","iopub.status.idle":"2025-11-24T06:44:12.830021Z","shell.execute_reply.started":"2025-11-24T06:44:12.821144Z","shell.execute_reply":"2025-11-24T06:44:12.829329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Normalizing the images to range from [0,1] instead of [0,255]\n\nX_train_scaled=X_train/255\nX_test_scaled=X_test/255\nX_val_scaled=X_val/255\n\nprint(np.max(X_train_scaled))\nprint(np.min(X_train_scaled))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:14.780757Z","iopub.execute_input":"2025-11-24T06:44:14.781444Z","iopub.status.idle":"2025-11-24T06:44:14.953753Z","shell.execute_reply.started":"2025-11-24T06:44:14.781420Z","shell.execute_reply":"2025-11-24T06:44:14.952929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Baseline CNN","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(learning_rate=1e-4))\nprint(model.summary())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:18.435727Z","iopub.execute_input":"2025-11-24T06:44:18.436336Z","iopub.status.idle":"2025-11-24T06:44:20.689078Z","shell.execute_reply.started":"2025-11-24T06:44:18.436312Z","shell.execute_reply":"2025-11-24T06:44:20.688246Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 50 \nbatch_size = 32 \n\nearly_stopping = EarlyStopping(patience=5, monitor='val_loss')  \n\n\nhistory = model.fit(X_train_scaled,\n                    y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_data=(X_val_scaled,y_val),\n                   callbacks=[early_stopping])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:44:23.861059Z","iopub.execute_input":"2025-11-24T06:44:23.861560Z","iopub.status.idle":"2025-11-24T06:44:45.253735Z","shell.execute_reply.started":"2025-11-24T06:44:23.861537Z","shell.execute_reply":"2025-11-24T06:44:45.253082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_val_scaled)\nthreshold = 0.5 \nbinary_predictions = (predictions > threshold).astype(int)\n\nconf_matrix = confusion_matrix(y_val, binary_predictions)\n\naccuracy = accuracy_score(y_val, binary_predictions)\nprint(\"Accuracy on Validation Set: {:.3f} %\".format(accuracy))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix For Validation')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:45:38.591329Z","iopub.execute_input":"2025-11-24T06:45:38.592139Z","iopub.status.idle":"2025-11-24T06:45:39.115529Z","shell.execute_reply.started":"2025-11-24T06:45:38.592112Z","shell.execute_reply":"2025-11-24T06:45:39.114896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_test_scaled)\nthreshold = 0.5 \nbinary_predictions = (predictions > threshold).astype(int)\n\nconf_matrix = confusion_matrix(y_test, binary_predictions)\n\naccuracy = accuracy_score(y_test, binary_predictions)\nprint(\"Accuracy on Test Set: {:.3f} %\".format(accuracy))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix For Test')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:45:41.925911Z","iopub.execute_input":"2025-11-24T06:45:41.926590Z","iopub.status.idle":"2025-11-24T06:45:43.243950Z","shell.execute_reply.started":"2025-11-24T06:45:41.926563Z","shell.execute_reply":"2025-11-24T06:45:43.243292Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:45:46.420673Z","iopub.execute_input":"2025-11-24T06:45:46.420969Z","iopub.status.idle":"2025-11-24T06:45:46.576693Z","shell.execute_reply.started":"2025-11-24T06:45:46.420947Z","shell.execute_reply":"2025-11-24T06:45:46.576099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:46:16.116132Z","iopub.execute_input":"2025-11-24T06:46:16.116718Z","iopub.status.idle":"2025-11-24T06:46:16.286313Z","shell.execute_reply.started":"2025-11-24T06:46:16.116692Z","shell.execute_reply":"2025-11-24T06:46:16.285582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Augumentation","metadata":{}},{"cell_type":"code","source":"# Using ImageDataGenerator to perform data augmentation on the training set. This helps increase the diversity of the training data and improve the model's generalization ability.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:46:26.096770Z","iopub.execute_input":"2025-11-24T06:46:26.097070Z","iopub.status.idle":"2025-11-24T06:46:26.100626Z","shell.execute_reply.started":"2025-11-24T06:46:26.097024Z","shell.execute_reply":"2025-11-24T06:46:26.099946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    shear_range=0.1,\n    brightness_range=[0.5, 1.5],\n    rescale=1./255\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\n\nimage_size=(224,224)\n\ntrain_generator = datagen.flow_from_directory(\n    train_folder,\n    color_mode='rgb',\n    target_size=image_size,\n    batch_size=32,\n    class_mode='binary'\n)\n\nval_generator = val_datagen.flow_from_directory(\n    val_folder,\n    color_mode='rgb',\n    target_size=image_size,\n    batch_size=32,  \n    class_mode='binary'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:48:19.282766Z","iopub.execute_input":"2025-11-24T06:48:19.283126Z","iopub.status.idle":"2025-11-24T06:48:19.303918Z","shell.execute_reply.started":"2025-11-24T06:48:19.283103Z","shell.execute_reply":"2025-11-24T06:48:19.303099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"augmented_images, _ = next(datagen.flow(X_train, y_train, batch_size=32))\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_images[i])\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:48:29.580790Z","iopub.execute_input":"2025-11-24T06:48:29.581093Z","iopub.status.idle":"2025-11-24T06:48:30.347123Z","shell.execute_reply.started":"2025-11-24T06:48:29.581073Z","shell.execute_reply":"2025-11-24T06:48:30.346220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VGG 16","metadata":{}},{"cell_type":"markdown","source":"Using a pre-trained VGG16 model with keras-pretrained-models weights, because it provides a well-established convolutional neural network architecture that has been trained on a large dataset (ImageNet). By using a pre-trained model like VGG16, we can benefit from the features learned during its training on ImageNet, which includes detecting various shapes, textures, and patterns in images.","metadata":{}},{"cell_type":"code","source":"base_model = VGG16(weights='/kaggle/input/keras-pretrained-models/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(224, 224, 3))\n\nfor layer in base_model.layers[:-5]:\n    layer.trainable = False\n\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)  \npredictions = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.compile(loss='binary_crossentropy', optimizer=RMSprop(learning_rate=1e-4), metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:48:32.540973Z","iopub.execute_input":"2025-11-24T06:48:32.541678Z","iopub.status.idle":"2025-11-24T06:48:33.398425Z","shell.execute_reply.started":"2025-11-24T06:48:32.541655Z","shell.execute_reply":"2025-11-24T06:48:33.397840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n\nhistory = model.fit(\n    train_generator,\n    epochs=200,\n    validation_data=val_generator,\n    callbacks=[early_stopping, reduce_lr]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:48:35.460773Z","iopub.execute_input":"2025-11-24T06:48:35.461411Z","iopub.status.idle":"2025-11-24T06:50:54.638048Z","shell.execute_reply.started":"2025-11-24T06:48:35.461385Z","shell.execute_reply":"2025-11-24T06:50:54.637408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_val_scaled)\nthreshold = 0.5 \nbinary_predictions = (predictions > threshold).astype(int)\n\nconf_matrix = confusion_matrix(y_val, binary_predictions)\n\naccuracy = accuracy_score(y_val, binary_predictions)\nprint(\"Accuracy on Validation Set: {:f}\".format(accuracy))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix For Validation')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:50:54.639276Z","iopub.execute_input":"2025-11-24T06:50:54.639493Z","iopub.status.idle":"2025-11-24T06:50:55.526716Z","shell.execute_reply.started":"2025-11-24T06:50:54.639476Z","shell.execute_reply":"2025-11-24T06:50:55.526083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_test_scaled)\nthreshold = 0.5 \nbinary_predictions = (predictions > threshold).astype(int)\n\nconf_matrix = confusion_matrix(y_test, binary_predictions)\n\naccuracy = accuracy_score(y_test, binary_predictions)\nprint(\"Accuracy on Test Set: {:f}\".format(accuracy))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix For Test')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:51:02.376024Z","iopub.execute_input":"2025-11-24T06:51:02.376864Z","iopub.status.idle":"2025-11-24T06:51:13.038709Z","shell.execute_reply.started":"2025-11-24T06:51:02.376831Z","shell.execute_reply":"2025-11-24T06:51:13.038089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:52:26.398800Z","iopub.execute_input":"2025-11-24T06:52:26.399429Z","iopub.status.idle":"2025-11-24T06:52:26.559879Z","shell.execute_reply.started":"2025-11-24T06:52:26.399405Z","shell.execute_reply":"2025-11-24T06:52:26.559308Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:52:30.176277Z","iopub.execute_input":"2025-11-24T06:52:30.177087Z","iopub.status.idle":"2025-11-24T06:52:30.343458Z","shell.execute_reply.started":"2025-11-24T06:52:30.177053Z","shell.execute_reply":"2025-11-24T06:52:30.342812Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Efficient Net","metadata":{}},{"cell_type":"code","source":"base_model = EfficientNetB0(weights=None, include_top=False, input_shape=(224,224,3))\n\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\nx = GlobalAveragePooling2D()(base_model.output)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = BatchNormalization()(x)\npredictions = Dense(1, activation='sigmoid')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=1e-4), metrics=['accuracy'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:52:33.231382Z","iopub.execute_input":"2025-11-24T06:52:33.231920Z","iopub.status.idle":"2025-11-24T06:52:34.092767Z","shell.execute_reply.started":"2025-11-24T06:52:33.231895Z","shell.execute_reply":"2025-11-24T06:52:34.091977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6)\n\nhistory = model.fit(\n    train_generator,\n    epochs=200,\n    validation_data=val_generator,\n    callbacks=[early_stopping, reduce_lr]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:52:36.490575Z","iopub.execute_input":"2025-11-24T06:52:36.491310Z","iopub.status.idle":"2025-11-24T06:55:40.861397Z","shell.execute_reply.started":"2025-11-24T06:52:36.491284Z","shell.execute_reply":"2025-11-24T06:55:40.860445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_val_scaled)\nthreshold = 0.5 \nbinary_predictions = (predictions > threshold).astype(int)\n\nconf_matrix = confusion_matrix(y_val, binary_predictions)\n\naccuracy = accuracy_score(y_val, binary_predictions)\nprint(\"Accuracy on Validation Set: {:f}\".format(accuracy))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix For Validation')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:56:07.857831Z","iopub.execute_input":"2025-11-24T06:56:07.858611Z","iopub.status.idle":"2025-11-24T06:56:12.969185Z","shell.execute_reply.started":"2025-11-24T06:56:07.858584Z","shell.execute_reply":"2025-11-24T06:56:12.968408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = model.predict(X_test_scaled)\nthreshold = 0.5 \nbinary_predictions = (predictions > threshold).astype(int)\n\nconf_matrix = confusion_matrix(y_test, binary_predictions)\n\naccuracy = accuracy_score(y_test, binary_predictions)\nprint(\"Accuracy on Test Set: {:f}\".format(accuracy))\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix For Test')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:56:17.874823Z","iopub.execute_input":"2025-11-24T06:56:17.875564Z","iopub.status.idle":"2025-11-24T06:56:25.097920Z","shell.execute_reply.started":"2025-11-24T06:56:17.875540Z","shell.execute_reply":"2025-11-24T06:56:25.097291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:56:31.088814Z","iopub.execute_input":"2025-11-24T06:56:31.089702Z","iopub.status.idle":"2025-11-24T06:56:31.272376Z","shell.execute_reply.started":"2025-11-24T06:56:31.089677Z","shell.execute_reply":"2025-11-24T06:56:31.271516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'], label='Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T06:56:34.989247Z","iopub.execute_input":"2025-11-24T06:56:34.989952Z","iopub.status.idle":"2025-11-24T06:56:35.151526Z","shell.execute_reply.started":"2025-11-24T06:56:34.989927Z","shell.execute_reply":"2025-11-24T06:56:35.150930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}